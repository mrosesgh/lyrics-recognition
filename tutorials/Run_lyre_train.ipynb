{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Run lyre.train.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVPOPBUTyViG"
   },
   "source": [
    "In this colab you will see how to install and train the lyre model (Demucs+Wav2Vec).\n",
    "\n",
    "First, we will have to install the packages needed to run the train"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-SRmN-NRSy6I",
    "outputId": "1fcda166-0ac0-4c27-cfc7-074110448348"
   },
   "source": [
    "!pip install https://github.com/kpu/kenlm/archive/master.zip"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/kpu/kenlm/archive/master.zip\n",
      "\u001B[?25l  Downloading https://github.com/kpu/kenlm/archive/master.zip\n",
      "\u001B[K     | 1.1MB 12.8MB/s\n",
      "\u001B[?25hBuilding wheels for collected packages: kenlm\n",
      "  Building wheel for kenlm (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for kenlm: filename=kenlm-0.0.0-cp37-cp37m-linux_x86_64.whl size=2336129 sha256=284a2dfa9edadb8b18e30d052544313540095dd4777fce3b601df0b86910aec6\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-erl5_eai/wheels/2d/32/73/e3093c9d11dc8abf79c156a4db1a1c5631428059d4f9ff2cba\n",
      "Successfully built kenlm\n",
      "Installing collected packages: kenlm\n",
      "Successfully installed kenlm-0.0.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E4x2f4E6-FNu",
    "outputId": "a1ae17cf-8d80-467c-e0c2-02a23e221a05"
   },
   "source": [
    "!git clone https://github.com/ynop/py-ctc-decode"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Cloning into 'py-ctc-decode'...\n",
      "remote: Enumerating objects: 28, done.\u001B[K\n",
      "remote: Counting objects: 100% (28/28), done.\u001B[K\n",
      "remote: Compressing objects: 100% (24/24), done.\u001B[K\n",
      "remote: Total 28 (delta 6), reused 17 (delta 2), pack-reused 0\u001B[K\n",
      "Unpacking objects: 100% (28/28), done.\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_7UUkQa-lw3"
   },
   "source": [
    "Before installing py-ctc-decode, we must remove the version dependencies in REQURED section from setup.py. Otherwise, it will install older versions of `numpy`, `tqdm` and `psutil`\n",
    "\n",
    "```python\n",
    "REQUIRED = [\n",
    "    'tqdm==4.39.0',\n",
    "    'kenlm @ git+ssh://git@github.com/kpu/kenlm@{}#egg=kenlm'.format(\n",
    "        KENLM_COMMIT\n",
    "    ),\n",
    "    'numpy==1.16.2',\n",
    "    'psutil==5.6.7',\n",
    "]\n",
    "```\n",
    "\n",
    "to\n",
    "\n",
    "```python\n",
    "REQUIRED = [\n",
    "    'tqdm',\n",
    "    'kenlm @ git+ssh://git@github.com/kpu/kenlm@master',\n",
    "    'numpy',\n",
    "    'psutil',\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZIxsvx84FfOq"
   },
   "source": [
    "!sed -i -e 's/tqdm==4\\.39\\.0/tqdm/g' -e 's/numpy==1\\.16\\.2/numpy/g' -e 's/psutil==5\\.6\\.7/psutil/g' py-ctc-decode/setup.py"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Od6Fs1f_M9iq"
   },
   "source": [
    "Once removed we can install the package"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1laAuSEeC51V",
    "outputId": "2934fd74-84e6-4a17-9c0f-93fe0987cff8"
   },
   "source": [
    "%cd py-ctc-decode\n",
    "!pip install .\n",
    "%cd -"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/content/py-ctc-decode\n",
      "Processing /content/py-ctc-decode\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from ctcdecode==0.0.0) (4.41.1)\n",
      "Requirement already satisfied: kenlm@ git+ssh://git@github.com/kpu/kenlm@96d303cfb1a0c21b8f060dbad640d7ab301c019a#egg=kenlm from git+ssh://****@github.com/kpu/kenlm@96d303cfb1a0c21b8f060dbad640d7ab301c019a#egg=kenlm in /usr/local/lib/python3.7/dist-packages (from ctcdecode==0.0.0) (0.0.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ctcdecode==0.0.0) (1.19.5)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from ctcdecode==0.0.0) (5.4.8)\n",
      "Building wheels for collected packages: ctcdecode\n",
      "  Building wheel for ctcdecode (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for ctcdecode: filename=ctcdecode-0.0.0-cp37-none-any.whl size=9275 sha256=078cd37b06ee78f6f61b78e6fac5978a95586d5c2899393e8699796754ee9c01\n",
      "  Stored in directory: /root/.cache/pip/wheels/b5/6a/d2/da6d2e5294501852bcb321a46602d1a684f0aee9e9be7cd24f\n",
      "Successfully built ctcdecode\n",
      "Installing collected packages: ctcdecode\n",
      "Successfully installed ctcdecode-0.0.0\n",
      "/root\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfvvn9xqNCgh"
   },
   "source": [
    "Now it is time to clone the most relevant code in this notebook, the **lyre** package"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OcQNkQzzQuAc",
    "outputId": "12d5a8cc-17d0-47f2-e5fd-4ad852b6c569"
   },
   "source": [
    "%rm -fr aidl-lyrics-recognition\n",
    "!git clone https://github.com/ttecles/aidl-lyrics-recognition\n",
    "%cd aidl-lyrics-recognition"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Cloning into 'aidl-lyrics-recognition'...\n",
      "remote: Enumerating objects: 375, done.\u001B[K\n",
      "remote: Counting objects: 100% (375/375), done.\u001B[K\n",
      "remote: Compressing objects: 100% (274/274), done.\u001B[K\n",
      "remote: Total 375 (delta 225), reused 205 (delta 93), pack-reused 0\u001B[K\n",
      "Receiving objects: 100% (375/375), 1.70 MiB | 12.03 MiB/s, done.\n",
      "Resolving deltas: 100% (225/225), done.\n",
      "/root/aidl-lyrics-recognition/aidl-lyrics-recognition\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3qCOJlauQ_RR",
    "outputId": "cbb45813-6af4-4827-dade-69f79046a0db"
   },
   "source": [
    "pip install -r requirements.txt"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.19.5)\n",
      "Collecting dali-dataset\n",
      "  Downloading https://files.pythonhosted.org/packages/9a/e8/e75fb4d9ab85e495aa7de120ff6cb0b78fab81d350f72d13a5fb0b506c07/DALI_dataset-1.1-py3-none-any.whl\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.9.0+cu102)\n",
      "Collecting torchaudio\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/a8/20/eab40caad8f4b97f5e91a5de8ba5ec29115e08fa4c9a808725490b7b4844/torchaudio-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (1.9MB)\n",
      "\u001B[K     |████████████████████████████████| 1.9MB 40.2MB/s \n",
      "\u001B[?25hCollecting wandb\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/d4/f6/91c07f54c2162854f5028aaa13f576ca17a3bc0cf6da02c2ad5baddae128/wandb-0.10.33-py2.py3-none-any.whl (1.8MB)\n",
      "\u001B[K     |████████████████████████████████| 1.8MB 37.6MB/s \n",
      "\u001B[?25hCollecting demucs\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/48/91/c6594847bc3ab99597f58acdb99c022cfe1d6027353e695778dd90301128/demucs-2.0.3.tar.gz (51kB)\n",
      "\u001B[K     |████████████████████████████████| 61kB 10.6MB/s \n",
      "\u001B[?25hCollecting transformers\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n",
      "\u001B[K     |████████████████████████████████| 2.5MB 45.6MB/s \n",
      "\u001B[?25hCollecting jiwer\n",
      "  Downloading https://files.pythonhosted.org/packages/8c/cc/fb9d3132cba1f6d393b7d5a9398d9d4c8fc033bc54668cf87e9b197a6d7a/jiwer-2.2.0-py3-none-any.whl\n",
      "Collecting python-dotenv\n",
      "  Downloading https://files.pythonhosted.org/packages/5c/0c/9c5d5dd254e9e7a32d34777cc6fd33cbeb174744061458b88470aecbd1d6/python_dotenv-0.18.0-py2.py3-none-any.whl\n",
      "Collecting accelerate\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/f7/fa/d173d923c953d930702066894abf128a7e5258c6f64cf088d2c5a83f46a3/accelerate-0.3.0-py3-none-any.whl (49kB)\n",
      "\u001B[K     |████████████████████████████████| 51kB 7.7MB/s \n",
      "\u001B[?25hRequirement already satisfied: soundfile in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (0.10.3.post1)\n",
      "Collecting youtube-dl\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/a4/43/1f586e49e68f8b41c4be416302bf96ddd5040b0e744b5902d51063795eb9/youtube_dl-2021.6.6-py2.py3-none-any.whl (1.9MB)\n",
      "\u001B[K     |████████████████████████████████| 1.9MB 43.9MB/s \n",
      "\u001B[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r requirements.txt (line 3)) (3.7.4.3)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (2.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (5.4.8)\n",
      "Collecting sentry-sdk>=0.4.0\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/ce/41/75fad31fff378871c462745ce724b3701a6acad17028d79476ec2545e40f/sentry_sdk-1.3.0-py2.py3-none-any.whl (133kB)\n",
      "\u001B[K     |████████████████████████████████| 143kB 56.9MB/s \n",
      "\u001B[?25hCollecting pathtools\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (1.15.0)\n",
      "Collecting subprocess32>=3.5.3\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
      "\u001B[K     |████████████████████████████████| 102kB 13.5MB/s \n",
      "\u001B[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (2.23.0)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (3.13)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (2.8.1)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (3.17.3)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (7.1.2)\n",
      "Collecting GitPython>=1.0.0\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/bc/91/b38c4fabb6e5092ab23492ded4f318ab7299b19263272b703478038c0fbc/GitPython-3.1.18-py3-none-any.whl (170kB)\n",
      "\u001B[K     |████████████████████████████████| 174kB 52.2MB/s \n",
      "\u001B[?25hCollecting configparser>=3.8.1\n",
      "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
      "Collecting diffq>=0.1\n",
      "  Downloading https://files.pythonhosted.org/packages/5a/4a/f9b69b5420f410667f6615bf8b63543638339f68b469500547a9520567d9/diffq-0.1.1.tar.gz\n",
      "Collecting lameenc>=1.2\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/ba/d7/7a701f6eefcbd5423751e253008967fbea9f3bc7f791400da704239314fe/lameenc-1.3.1-cp37-cp37m-manylinux1_x86_64.whl (187kB)\n",
      "\u001B[K     |████████████████████████████████| 194kB 54.4MB/s \n",
      "\u001B[?25hCollecting julius>=0.2.3\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/d3/4f/19c160cf882ce1b0fc39af5f41471ec565bdc0e1fa050408ed3ca7c836dc/julius-0.2.4.tar.gz (54kB)\n",
      "\u001B[K     |████████████████████████████████| 61kB 10.3MB/s \n",
      "\u001B[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from demucs->-r requirements.txt (line 6)) (4.41.1)\n",
      "Collecting sacremoses\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
      "\u001B[K     |████████████████████████████████| 901kB 41.2MB/s \n",
      "\u001B[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 7)) (3.0.12)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 7)) (20.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 7)) (2019.12.20)\n",
      "Collecting huggingface-hub==0.0.12\n",
      "  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 7)) (4.6.0)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
      "\u001B[K     |████████████████████████████████| 3.3MB 41.8MB/s \n",
      "\u001B[?25hCollecting python-Levenshtein\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/2a/dc/97f2b63ef0fa1fd78dcb7195aca577804f6b2b51e712516cc0e902a9a201/python-Levenshtein-0.12.2.tar.gz (50kB)\n",
      "\u001B[K     |████████████████████████████████| 51kB 8.9MB/s \n",
      "\u001B[?25hCollecting pyaml>=20.4.0\n",
      "  Downloading https://files.pythonhosted.org/packages/15/c4/1310a054d33abc318426a956e7d6df0df76a6ddfa9c66f6310274fb75d42/pyaml-20.4.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile->-r requirements.txt (line 11)) (1.14.5)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb->-r requirements.txt (line 5)) (2021.5.30)\n",
      "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb->-r requirements.txt (line 5)) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 5)) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 5)) (3.0.4)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
      "\u001B[K     |████████████████████████████████| 71kB 12.3MB/s \n",
      "\u001B[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->-r requirements.txt (line 7)) (1.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers->-r requirements.txt (line 7)) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers->-r requirements.txt (line 7)) (3.4.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein->jiwer->-r requirements.txt (line 8)) (57.0.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile->-r requirements.txt (line 11)) (2.20)\n",
      "Collecting smmap<5,>=3.0.1\n",
      "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: demucs, pathtools, subprocess32, diffq, julius, python-Levenshtein\n",
      "  Building wheel for demucs (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for demucs: filename=demucs-2.0.3-cp37-none-any.whl size=44136 sha256=63aabd56e1a9c8fe5380b1ddd0162483b82cbf81559b29bf04e91e2f24c19f72\n",
      "  Stored in directory: /root/.cache/pip/wheels/c4/a0/d0/c7f2ac6ffd8888369c21318e24d418cac3fa08309125f515e0\n",
      "  Building wheel for pathtools (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8807 sha256=45363a18e75da8810cca201841aa5e21731a0a1d63e00aa35a2be2ef85e6a005\n",
      "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
      "  Building wheel for subprocess32 (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6502 sha256=2fcdab8a77a843d16e3a23fcb08ef59b7b465dfdb80597e90d7bf4e709e65186\n",
      "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
      "  Building wheel for diffq (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for diffq: filename=diffq-0.1.1-cp37-none-any.whl size=18978 sha256=2d3a4e7ceb7847af0a8f64bdb4dec7a86c1dfa0aeb30772d4eb19ac93d1eb67d\n",
      "  Stored in directory: /root/.cache/pip/wheels/21/d3/2d/563bdfe4d3ab1f9fa5798e8f545aab1bf666ec2f2177802eac\n",
      "  Building wheel for julius (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for julius: filename=julius-0.2.4-cp37-none-any.whl size=20345 sha256=c52ac2bce3441b24ac6163ecb4621a16a6e8198e0a66214bc6283fb0f2cb4b96\n",
      "  Stored in directory: /root/.cache/pip/wheels/aa/cb/8d/dc01c714d143f437d0540de58b81bec07631f208f559855afd\n",
      "  Building wheel for python-Levenshtein (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=149810 sha256=66277ba053261239a1719d881770ff5c5299eec35a4b4db516c99ffb7ff46acd\n",
      "  Stored in directory: /root/.cache/pip/wheels/b3/26/73/4b48503bac73f01cf18e52cd250947049a7f339e940c5df8fc\n",
      "Successfully built demucs pathtools subprocess32 diffq julius python-Levenshtein\n",
      "Installing collected packages: youtube-dl, dali-dataset, torchaudio, sentry-sdk, pathtools, subprocess32, smmap, gitdb, GitPython, configparser, docker-pycreds, shortuuid, wandb, diffq, lameenc, julius, demucs, sacremoses, huggingface-hub, tokenizers, transformers, python-Levenshtein, jiwer, python-dotenv, pyaml, accelerate\n",
      "Successfully installed GitPython-3.1.18 accelerate-0.3.0 configparser-5.0.2 dali-dataset-1.1 demucs-2.0.3 diffq-0.1.1 docker-pycreds-0.4.0 gitdb-4.0.7 huggingface-hub-0.0.12 jiwer-2.2.0 julius-0.2.4 lameenc-1.3.1 pathtools-0.1.2 pyaml-20.4.0 python-Levenshtein-0.12.2 python-dotenv-0.18.0 sacremoses-0.0.45 sentry-sdk-1.3.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 tokenizers-0.10.3 torchaudio-0.9.0 transformers-4.8.2 wandb-0.10.33 youtube-dl-2021.6.6\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JrdWCPiCy0e5"
   },
   "source": [
    "Nou we will mount the drive where the DALI dataset lives"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZoYOTrcWRQTA",
    "outputId": "2283e6fd-5587-45d5-c93c-03352c794fb5"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0Ye3WadI61eA"
   },
   "source": [
    "import os"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhMGz9XeDlry"
   },
   "source": [
    "Let's load all the vars needed for register information into Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "D3N08at0RzeG"
   },
   "source": [
    "env = dict(\n",
    "    WANDB_KEY=\"PROJECT\",\n",
    "    WANDB_PROJECT=\"demucs_wav2vec\",\n",
    "    WANDB_ENTITY=\"\",\n",
    "    WANDB_NAME=\"colab run\",\n",
    "    WANDB_MODE=\"online\",)\n",
    "os.environ.update(env)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FlXugdxnSJWH",
    "outputId": "f7ba006e-2e90-4444-a7b1-e86a0557f389"
   },
   "source": [
    "!python -m lyre.train -h"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "usage: train [-h] [--data-path DATA_PATH] [--dali-gt-file DALI_GT_FILE]\n",
      "             [--blacklist-file BLACKLIST_FILE] [--lm LM] [--ncc NCC]\n",
      "             [--train-split TRAIN_SPLIT] [--audio-length AUDIO_LENGTH]\n",
      "             [--stride STRIDE] [--epochs EPOCHS] [--batch BATCH]\n",
      "             [--optimizer {adam,sgd}] [--lr LR] [--wd WD] [--fp16 | --cpu]\n",
      "             [--workers WORKERS] [--demucs DEMUCS] [--wav2vec WAV2VEC]\n",
      "             [--tokenizer TOKENIZER] [--freeze-demucs] [--freeze-extractor]\n",
      "             [--load-model LOAD_MODEL] [--model-folder MODEL_FOLDER]\n",
      "             [--save-on-epoch SAVE_ON_EPOCH]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "\n",
      "data arguments:\n",
      "  data input information\n",
      "\n",
      "  --data-path DATA_PATH\n",
      "                        Path holding all the data.\n",
      "  --dali-gt-file DALI_GT_FILE\n",
      "                        DALI ground truth file.\n",
      "  --blacklist-file BLACKLIST_FILE\n",
      "                        All the identifiers listed in the file will be skipped\n",
      "                        from being loaded.\n",
      "  --lm LM               Trained Language Model file.\n",
      "\n",
      "train config arguments:\n",
      "  configuration of the training.\n",
      "\n",
      "  --ncc NCC             Train only with files with NCC score bigger than NCC.\n",
      "  --train-split TRAIN_SPLIT\n",
      "                        Train proportion. Requires --ncc to be specified.\n",
      "  --audio-length AUDIO_LENGTH\n",
      "                        Audio length in seconds to pass to the model.\n",
      "  --stride STRIDE       Stride used for spliting the audio songs.\n",
      "  --epochs EPOCHS       Number of epochs during training.\n",
      "  --batch BATCH         Batch size.\n",
      "  --optimizer {adam,sgd}\n",
      "                        Type of optimizer.\n",
      "  --lr LR               Optimizer learning rate.\n",
      "  --wd WD               Optimizer weight decay.\n",
      "  --fp16                If passed, will use FP16 training.\n",
      "  --cpu                 If passed, will train on the CPU.\n",
      "  --workers WORKERS     Number of workers used for processing chunks,\n",
      "                        DataLoader and decoder.\n",
      "\n",
      "model config arguments:\n",
      "  configuration of the model\n",
      "\n",
      "  --demucs DEMUCS       Name of the pretrained demucs.\n",
      "  --wav2vec WAV2VEC     Name of the pretrained wav2vec.\n",
      "  --tokenizer TOKENIZER\n",
      "                        Name of the pretrained tokenizer.\n",
      "  --freeze-demucs       Does not compute gradient on demucs model\n",
      "  --freeze-extractor    Freeze feature extractor layers from wav2vec.\n",
      "\n",
      "model IO arguments:\n",
      "  parameters related with load/save model\n",
      "\n",
      "  --load-model LOAD_MODEL\n",
      "                        Loads the specified model.\n",
      "  --model-folder MODEL_FOLDER\n",
      "                        Folder where the model will be saved per epoch and\n",
      "                        when signaled with SIGUSR.\n",
      "  --save-on-epoch SAVE_ON_EPOCH\n",
      "                        If specified, saves the model on every epoch.\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_098fCgtzAzX"
   },
   "source": [
    "Now it is time to train the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0_ung2v7t1b",
    "outputId": "f7d9ab19-88b6-4a03-f346-fb972e95a778"
   },
   "source": [
    "!python -m lyre.train --data-path /content/drive/MyDrive/UPC-AIDL-Spring2021/data/subset \\\n",
    "    --lm /content/drive/MyDrive/UPC-AIDL-Spring2021/data/text.arpa --epoch 1 --freeze-demucs --freeze-extractor \\\n",
    "    --model-folder /tmp"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Loading DALI dataset...\n",
      "Preparing Datasets...\n",
      "Train DaliDataset: 30 chunks\n",
      "Validation DaliDataset: 29 chunks\n",
      "Test DaliDataset: 34 chunks\n",
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /content/drive/MyDrive/UPC-AIDL-Spring2021/data/text.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "found 1gram\n",
      "found 2gram\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mttecles\u001B[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "2021-07-11 19:57:06.956333: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Tracking run with wandb version 0.10.33\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Syncing run \u001B[33mcolab run\u001B[0m\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: ⭐️ View project at \u001B[34m\u001B[4mhttps://wandb.ai/aidl-lyrics-recognition/demucs%2Bwav2vec\u001B[0m\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: 🚀 View run at \u001B[34m\u001B[4mhttps://wandb.ai/aidl-lyrics-recognition/demucs%2Bwav2vec/runs/tj0g9yzi\u001B[0m\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Run data is saved locally in /root/aidl-lyrics-recognition/wandb/run-20210711_195704-tj0g9yzi\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Start Training with device cuda\n",
      "2021-07-11 19:57:54.783156: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "EPOCH: 1,  | time in 0 minutes, 27 seconds\n",
      "=> train loss: 1.457  => valid loss: 0.899\n",
      "Training finished\n",
      "Saved model in /tmp/model_1626033489.pt\n",
      "100% 8/8 [03:25<00:00, 25.70s/it]\n",
      "100% 8/8 [01:21<00:00, 10.24s/it]\n",
      "100% 8/8 [03:27<00:00, 25.99s/it]\n",
      "100% 8/8 [01:26<00:00, 10.81s/it]\n",
      "100% 8/8 [03:10<00:00, 23.76s/it]\n",
      "100% 8/8 [01:16<00:00,  9.56s/it]\n",
      "100% 8/8 [03:26<00:00, 25.87s/it]\n",
      "100% 8/8 [01:16<00:00,  9.59s/it]\n",
      "100% 2/2 [01:04<00:00, 32.13s/it]\n",
      "100% 2/2 [00:17<00:00,  8.86s/it]\n",
      "Test set: Average loss: 1.0283, Wer: 84.36%, Beam Wer: 79.22%, Beam LM Wer: 76.60%\n",
      "\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Waiting for W&B process to finish, PID 921\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Program ended successfully.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:                                                                                \n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Find user logs for this run at: /root/aidl-lyrics-recognition/wandb/run-20210711_195704-tj0g9yzi/logs/debug.log\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Find internal logs for this run at: /root/aidl-lyrics-recognition/wandb/run-20210711_195704-tj0g9yzi/logs/debug-internal.log\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Run summary:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:   batch_train_loss 1.39038\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:           _runtime 1315\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:         _timestamp 1626034739\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:              _step 6\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:              epoch 0\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:         train_loss 1.45681\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:         valid_loss 0.89879\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:                wer 84.36232\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:           beam wer 79.22244\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:        beam lm wer 76.59873\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:          test_loss 1.0283\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Run history:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:   batch_train_loss █▁▄▄\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:           _runtime ▁▁▁▁▁▁█\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:         _timestamp ▁▁▁▁▁▁█\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:              _step ▁▂▃▅▆▇█\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:              epoch ▁\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:         train_loss ▁\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:         valid_loss ▁\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Synced 5 W&B file(s), 5 media file(s), 1 artifact file(s) and 0 other file(s)\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Synced \u001B[33mcolab run\u001B[0m: \u001B[34mhttps://wandb.ai/aidl-lyrics-recognition/demucs%2Bwav2vec/runs/tj0g9yzi\u001B[0m\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ehp907dYKEeV"
   },
   "source": [
    "You can send a `kill -10 PID` during training and it will save the weights into the model folder "
   ]
  }
 ]
}